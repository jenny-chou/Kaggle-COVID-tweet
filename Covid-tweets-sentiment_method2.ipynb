{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: COVID Tweet Sentiment Classification - Alternative Approach\n",
    "\n",
    "This is a solution for Kaggle project: https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n",
    "\n",
    "This notebook takes different NLP approach based on observations in Covid-tweets-sentiment_method1 notebook.\n",
    "\n",
    "Twitter is a place to express thoughts and ideas and spread news and information, thus makes it a great source to track trends worldwide. Through the collected tweets, we can evaluate people's attitude toward Covid and pandemic through the trending keywords and tags through time. The estimator can be useful for identifying inappropriate tweets and stop toxic propaganda or evaluating strategies to promote social distancing using trend words.\n",
    "\n",
    "The task is to classify tweet sentiments in given tweets.\n",
    "\n",
    "This notebook takes the approach of training multiple classifiers to identify tweet sentiment:\n",
    "1. Sentiment classifier: combine extremely positive with positive and extremely negative with negative, and classify positive, negative, and neutral tweets.\n",
    "2. Extremely positive classifier: from the predicted positive tweets, classify extreme tweets.\n",
    "3. Extremely negative classifier: from the predicted negative tweets, classify extreme tweets.\n",
    "\n",
    "Data has following columns:\n",
    "\n",
    "|Column |Notes |\n",
    "|:------|:-------|\n",
    "UserName | User ID\n",
    "ScreenName| User display name\n",
    "Location | User location\n",
    "TweetAt | Date\n",
    "OriginalTweet | Tweet content\n",
    "Sentiment | Extremely negative, Negative, Neutral, Positive, Extremely positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Corona_NLP_train.csv\", header=0, encoding='latin1')\n",
    "test_df = pd.read_csv(\"Corona_NLP_test.csv\", header=0, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "### Handle useless column\n",
    "Both UserName and ScreenName identify a user. We don't need both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=\"ScreenName\", inplace=True)\n",
    "test_df.drop(columns=\"ScreenName\", inplace=True)\n",
    "train_df.set_index(\"UserName\", inplace=True)\n",
    "test_df.set_index(\"UserName\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode sentiment\n",
    "Two encoders needed:\n",
    "1. Sentiment encoder: encodes Extremely Negative with same label as Negative and Extremely Positive with same label as Positive\n",
    "2. Extreme text encoder: encodes extreme tweets as 1 and regular tweets as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_encoder = {'Extremely Negative': 0,\n",
    "                     'Negative': 0,\n",
    "                     'Neutral': 1, \n",
    "                     'Positive': 2,\n",
    "                     'Extremely Positive': 2}\n",
    "           \n",
    "train_df[\"Sentiment_encode\"] = train_df.Sentiment.map(sentiment_encoder)\n",
    "test_df[\"Sentiment_encode\"] = test_df.Sentiment.map(sentiment_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_txt_encoder = {'Extremely Negative': 1,\n",
    "                      'Negative': 0,\n",
    "                      'Neutral': 0,  \n",
    "                      'Positive': 0,\n",
    "                      'Extremely Positive': 1}\n",
    "\n",
    "train_df[\"Extreme_txt_encode\"] = train_df.Sentiment.map(extreme_txt_encoder)\n",
    "test_df[\"Extreme_txt_encode\"] = test_df.Sentiment.map(extreme_txt_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Account Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep most frquent @:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = []\n",
    "for tweet in train_df.index.values:\n",
    "    tag_list += re.findall(r'@\\w+', train_df.loc[tweet, \"OriginalTweet\"])\n",
    "    \n",
    "tag_df = pd.DataFrame(tag_list)\n",
    "tag_sort = pd.DataFrame(tag_df.value_counts(ascending=False))\n",
    "\n",
    "top_tags = [acct[0] for acct in tag_sort.index.values[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep most frequent hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hash_list = []\n",
    "for tweet in train_df.index.values:\n",
    "    hash_list += re.findall(r'#\\w+', train_df.loc[tweet, \"OriginalTweet\"])\n",
    "    \n",
    "hash_df = pd.DataFrame(hash_list)\n",
    "hash_sort = pd.DataFrame(hash_df.value_counts(ascending=False))\n",
    "\n",
    "top_hashtags = [tag[0] for tag in hash_sort.index.values[:30] \n",
    "                if \"covid\" not in tag[0].lower() and \"corona\" not in tag[0].lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jenny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = list(set(nltk.corpus.stopwords.words('english')))\n",
    "stop_words += [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "               \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "stop_words += [\"in\", \"on\", \"at\", \"via\", \"due\", \"one\", \"two\", \"etc\", \"per\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine tokens\n",
    "After one round of training, examine and normalize tokens. \n",
    "\n",
    "1. Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n",
    "    - Example: running → run\n",
    "2. Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma.\n",
    "    - Example: better → good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_dict = {\"buying\":\"buy\", \"bought\":\"buy\", \"working\":\"work\", \"worked\":\"work\", \"shopping\":\"shop\", \"shopped\":\"shop\",\n",
    "                 \"shops\":\"shop\", \"days\":\"day\", \"weeks\":\"week\", \"masks\":\"mask\", \"got\":\"get\", \"gets\":\"get\", \n",
    "                 \"supermarkets\":\"supermarket\", \"says\":\"say\", \"saying\":\"say\", \"said\":\"say\", \"getting\":\"get\", \"gets\":\"get\", \n",
    "                 \"got\":\"get\", \"making\":\"make\", \"made\":\"make\", \"services\":\"service\", \"hours\":\"hour\", \"years\":\"year\", \n",
    "                 \"increases\":\"increase\", \"increased\":\"increase\", \"markets\":\"market\", \"close\":\"closed\", \"needs\":\"need\",\n",
    "                 \"needed\":\"need\", \"hands\":\"hand\", \"stores\":\"store\", \"employees\":\"employee\", \"workers\":\"worker\", \n",
    "                 \"staffs\":\"staff\", \"businesses\":\"business\", \"companies\":\"company\", \"consumers\":\"consumer\", \n",
    "                 \"customers\":\"customer\", \"products\":\"product\", \"going\":\"go\", \"went\":\"go\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization_dict = {\"shop\":\"store\", \"supermarket\":\"market\", \"much\":\"many\", \"employee\":\"worker\", \"staff\":\"worker\", \n",
    "                      \"global\":\"world\", \"company\":\"business\", \"consumer\":\"customer\", \"house\":\"home\", \"grocery\":\"goods\",\n",
    "                      \"products\":\"goods\", \"toilet\":\"toiletpaper\", \"paper\":\"toiletpaper\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below files are used for data cleaning - Retrieving countries with help of city codes/state codes...(optional)\n",
    "states = json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/states.json\").text)\n",
    "countries=json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/countries.json\").text)\n",
    "cities=json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/cities.json\").text)\n",
    "us_states_code=pd.read_csv('https://worldpopulationreview.com/static/states/abbr-name.csv',names=['state_code','state'])\n",
    "\n",
    "states = states[\"states\"]\n",
    "countries = countries[\"countries\"]\n",
    "cities = cities[\"cities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "world_dict = {}\n",
    "    \n",
    "for city in cities:\n",
    "    state_id = int(re.findall(r'\\d+', city[\"state_id\"])[0])\n",
    "    country_id = states[state_id-1][\"country_id\"]\n",
    "    country_id = int(re.findall(r'\\d+', country_id)[0])\n",
    "    country_name = countries[country_id-1][\"sortname\"]\n",
    "    world_dict[city[\"name\"].lower()] = country_name.lower()\n",
    "    \n",
    "for state in states:\n",
    "    country_id = int(re.findall(r'\\d+', state[\"country_id\"])[0])\n",
    "    country_name = countries[country_id-1][\"sortname\"]\n",
    "    world_dict[state[\"name\"].lower()] = country_name.lower()\n",
    "    \n",
    "for country in countries:    \n",
    "    world_dict[country[\"sortname\"].lower()] = country[\"sortname\"].lower()\n",
    "    world_dict[country[\"name\"].lower()] = country[\"sortname\"].lower()\n",
    "    \n",
    "for index in us_states_code.index:\n",
    "    state = us_states_code.loc[index]\n",
    "    world_dict[state.state_code.lower()] = \"us\"\n",
    "    world_dict[state.state.lower()] = \"us\"\n",
    "    \n",
    "world_dict[\"uk\"] = \"gb\"\n",
    "world_dict[\"ny\"] = \"us\"\n",
    "world_dict[\"nyc\"] = \"us\"\n",
    "world_dict[\"la\"] = \"us\"\n",
    "world_dict[\"sf\"] = \"us\"\n",
    "world_dict[\"bc\"] = \"ca\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean sentence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_link(sentence):\n",
    "    return re.sub(r'http\\S+', \" \", sentence)\n",
    "\n",
    "def remove_tag(sentence):\n",
    "    keep_tag = []\n",
    "    for tag in top_tags:\n",
    "        keep_tag += re.findall(tag, sentence)        \n",
    "    sentence = re.sub(r'@\\S+', \" \", sentence)    \n",
    "    for tag in keep_tag:\n",
    "        sentence += \" \" + tag        \n",
    "    return sentence\n",
    "    \n",
    "def remove_hashtag(sentence):\n",
    "    keep_tag = []\n",
    "    for tag in top_hashtags:\n",
    "        keep_tag += re.findall(tag, sentence)        \n",
    "    sentence = re.sub(r'#\\S+', \" \", sentence)    \n",
    "    for tag in keep_tag:\n",
    "        sentence += \" \" + tag        \n",
    "    return sentence\n",
    "    \n",
    "def remove_special_char(sentence):\n",
    "    for special_ch in (string.punctuation + \"0123456789\" + \"\\r\" + \"\\n\"):\n",
    "        sentence = sentence.replace(special_ch, \" \")\n",
    "    return sentence\n",
    "\n",
    "def remove_non_english(sentence):\n",
    "    clean_sentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        clean_sentence += str(np.where(word.isalpha(), (word + \" \"), \"\"))\n",
    "    return clean_sentence\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    for word in [\"covid\", \"corona\", \"virus\"]:\n",
    "        sentence = re.sub(word, \" \", sentence)\n",
    "    sentence = [word for word in sentence.split() if word not in stop_words]\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "def stemming_and_lemmatization(sentence):\n",
    "    clean_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word in stemming_dict:\n",
    "            word = stemming_dict[word]\n",
    "        if word in lemmatization_dict:\n",
    "            word = lemmatization_dict[word]\n",
    "        clean_sentence.append(word)    \n",
    "    return \" \".join(clean_sentence)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = remove_link(sentence)\n",
    "    sentence = remove_tag(sentence)\n",
    "    sentence = remove_hashtag(sentence)\n",
    "    sentence = remove_special_char(sentence)\n",
    "    sentence = remove_non_english(sentence)\n",
    "    sentence = remove_stop_words(sentence)\n",
    "    sentence = stemming_and_lemmatization(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean OriginalTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_original_tweet(df):\n",
    "    clean_tweet = []\n",
    "    loc_final_list = []\n",
    "\n",
    "    for tweet in df.index.values:\n",
    "        # clean location\n",
    "        loc_final, loc_clean = \"\", \"\"\n",
    "        if not pd.isnull(df.Location.loc[tweet]):\n",
    "            loc_clean = df.Location.astype(str).loc[tweet].lower()\n",
    "            loc_clean = clean_sentence(loc_clean)\n",
    "            for sub in loc_clean.split():\n",
    "                if sub in world_dict:\n",
    "                    loc_final = world_dict[sub]\n",
    "                    break\n",
    "            # add to list if not empty\n",
    "            if loc_final:\n",
    "                loc_final_list.append(loc_final)\n",
    "\n",
    "        # clean message\n",
    "        msg = df.OriginalTweet.astype(str).loc[tweet].lower()\n",
    "        msg = clean_sentence(msg)\n",
    "\n",
    "        # combine location and message to one sentence\n",
    "        clean_tweet.append(loc_final + \" \" + msg)\n",
    "\n",
    "    df['CleanTweet'] = clean_tweet\n",
    "    \n",
    "    return clean_tweet, loc_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_tweet, loc_final_list = clean_original_tweet(train_df)\n",
    "dummy = clean_original_tweet(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>Extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to exchange phone numbers create contact list with phone n...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>gb advice talk neighbours family exchange phone numbers create contact list phone numbers neighb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>australia woolworths give elderly disabled dedicated store hour amid outbreak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL B...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>food stock empty please panic enough food everyone take need stay calm stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ready go market outbreak paranoid food stock litteraly empty serious thing please panic causes ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Location     TweetAt  \\\n",
       "UserName                          \n",
       "3799         London  16-03-2020   \n",
       "3800             UK  16-03-2020   \n",
       "3801      Vagabonds  16-03-2020   \n",
       "3802            NaN  16-03-2020   \n",
       "3803            NaN  16-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "3799      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://...   \n",
       "3800      advice Talk to your neighbours family to exchange phone numbers create contact list with phone n...   \n",
       "3801      Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-...   \n",
       "3802      My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL B...   \n",
       "3803      Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid...   \n",
       "\n",
       "                   Sentiment  Sentiment_encode  Extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "3799                 Neutral                 1                   0   \n",
       "3800                Positive                 2                   0   \n",
       "3801                Positive                 2                   0   \n",
       "3802                Positive                 2                   0   \n",
       "3803      Extremely Negative                 0                   1   \n",
       "\n",
       "                                                                                                   CleanTweet  \n",
       "UserName                                                                                                       \n",
       "3799                                                                                                      gb   \n",
       "3800      gb advice talk neighbours family exchange phone numbers create contact list phone numbers neighb...  \n",
       "3801                            australia woolworths give elderly disabled dedicated store hour amid outbreak  \n",
       "3802                         food stock empty please panic enough food everyone take need stay calm stay safe  \n",
       "3803       ready go market outbreak paranoid food stock litteraly empty serious thing please panic causes ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>Extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>us find hand sanitizer fred meyer turned pack purell check concerns driving prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and loved ones from #coronavirus. ?</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>find protect loved ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;amp;medical supplies after...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Location     TweetAt  \\\n",
       "UserName                                    \n",
       "1                         NYC  02-03-2020   \n",
       "2                 Seattle, WA  02-03-2020   \n",
       "3                         NaN  02-03-2020   \n",
       "4                 Chicagoland  02-03-2020   \n",
       "5         Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...   \n",
       "2         When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...   \n",
       "3                                   Find out how you can protect yourself and loved ones from #coronavirus. ?   \n",
       "4         #Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after...   \n",
       "5         #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...   \n",
       "\n",
       "                   Sentiment  Sentiment_encode  Extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "1         Extremely Negative                 0                   1   \n",
       "2                   Positive                 2                   0   \n",
       "3         Extremely Positive                 2                   1   \n",
       "4                   Negative                 0                   0   \n",
       "5                    Neutral                 1                   0   \n",
       "\n",
       "                                                                                                   CleanTweet  \n",
       "UserName                                                                                                       \n",
       "1         us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...  \n",
       "2                          us find hand sanitizer fred meyer turned pack purell check concerns driving prices  \n",
       "3                                                                                     find protect loved ones  \n",
       "4          buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...  \n",
       "5                 au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of length of tweets')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_tweet = [len(tweet.split()) for tweet in train_df.CleanTweet]\n",
    "plt.figure()\n",
    "plt.hist(len_tweet, bins=20)\n",
    "plt.title(\"Histogram of length of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of length of tweets')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_tweet = [len(tweet.split()) for tweet in test_df.CleanTweet]\n",
    "plt.figure()\n",
    "plt.hist(len_tweet, bins=20)\n",
    "plt.title(\"Histogram of length of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classifier\n",
    "Classify if tweet is positive, neutral, or negative regardless if tweet uses extreme/drastic tones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32223\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df.CleanTweet.values)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)\n",
    "\n",
    "max_len = 25\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_df.CleanTweet.values)\n",
    "train_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test_df.CleanTweet.values)\n",
    "test_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_df.Sentiment_encode.values\n",
    "test_target = test_df.Sentiment_encode.values\n",
    "target_len = len(set(sentiment_encoder.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41157, 25), (41157,), (3798, 25), (3798,), 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus.shape, train_target.shape, test_corpus.shape, test_target.shape, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x2023e8633c8>,\n",
       "  <matplotlib.axis.XTick at 0x20239a830c8>,\n",
       "  <matplotlib.axis.XTick at 0x2023ec53208>,\n",
       "  <matplotlib.axis.XTick at 0x20239a7e408>,\n",
       "  <matplotlib.axis.XTick at 0x20239a81a08>,\n",
       "  <matplotlib.axis.XTick at 0x2023eca6248>,\n",
       "  <matplotlib.axis.XTick at 0x20239a7b808>,\n",
       "  <matplotlib.axis.XTick at 0x2023ebef888>,\n",
       "  <matplotlib.axis.XTick at 0x20239a82288>,\n",
       "  <matplotlib.axis.XTick at 0x20239a8bac8>,\n",
       "  <matplotlib.axis.XTick at 0x2023ecc0108>],\n",
       " [Text(0.0, 0, 'Negative'),\n",
       "  Text(0.2, 0, ''),\n",
       "  Text(0.4, 0, ''),\n",
       "  Text(0.6, 0, ''),\n",
       "  Text(0.8, 0, ''),\n",
       "  Text(1.0, 0, 'Neutral'),\n",
       "  Text(1.2, 0, ''),\n",
       "  Text(1.4, 0, ''),\n",
       "  Text(1.6, 0, ''),\n",
       "  Text(1.8, 0, ''),\n",
       "  Text(2.0, 0, 'Positive')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_target)\n",
    "plt.title(\"Number of tweets\")\n",
    "plt.xticks([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ], \n",
    "           [\"Negative\", \"\", \"\", \"\", \"\", \"Neutral\", \"\", \"\", \"\", \"\", \"Positive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41157 samples, validate on 3798 samples\n",
      "Epoch 1/3\n",
      "41157/41157 [==============================] - 39s 945us/sample - loss: 0.6548 - accuracy: 0.7107 - val_loss: 0.4705 - val_accuracy: 0.8294\n",
      "Epoch 2/3\n",
      "41157/41157 [==============================] - 33s 798us/sample - loss: 0.3401 - accuracy: 0.8872 - val_loss: 0.4320 - val_accuracy: 0.8478\n",
      "Epoch 3/3\n",
      "41157/41157 [==============================] - 34s 818us/sample - loss: 0.2642 - accuracy: 0.9150 - val_loss: 0.4547 - val_accuracy: 0.8444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x202d1dd2a48>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_dim = 32\n",
    "batch = 32\n",
    "epoch = 3\n",
    "\n",
    "sentiment_classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embd_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embd_dim*8)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(target_len, activation=tf.nn.sigmoid)\n",
    "])\n",
    "sentiment_classifier.compile(optimizer='adam', loss=tf.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "sentiment_classifier.fit(train_corpus, train_target, epochs=epoch, batch_size=batch, validation_data=(test_corpus, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>Extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>Sentiment_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>us find hand sanitizer fred meyer turned pack purell check concerns driving prices</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and loved ones from #coronavirus. ?</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>find protect loved ones</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;amp;medical supplies after...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Location     TweetAt  \\\n",
       "UserName                                    \n",
       "1                         NYC  02-03-2020   \n",
       "2                 Seattle, WA  02-03-2020   \n",
       "3                         NaN  02-03-2020   \n",
       "4                 Chicagoland  02-03-2020   \n",
       "5         Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...   \n",
       "2         When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...   \n",
       "3                                   Find out how you can protect yourself and loved ones from #coronavirus. ?   \n",
       "4         #Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after...   \n",
       "5         #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...   \n",
       "\n",
       "                   Sentiment  Sentiment_encode  Extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "1         Extremely Negative                 0                   1   \n",
       "2                   Positive                 2                   0   \n",
       "3         Extremely Positive                 2                   1   \n",
       "4                   Negative                 0                   0   \n",
       "5                    Neutral                 1                   0   \n",
       "\n",
       "                                                                                                   CleanTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...   \n",
       "2                          us find hand sanitizer fred meyer turned pack purell check concerns driving prices   \n",
       "3                                                                                     find protect loved ones   \n",
       "4          buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...   \n",
       "5                 au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper   \n",
       "\n",
       "          Sentiment_pred  \n",
       "UserName                  \n",
       "1                      0  \n",
       "2                      2  \n",
       "3                      2  \n",
       "4                      0  \n",
       "5                      1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = np.argmax(sentiment_classifier.predict(test_corpus), axis=1)\n",
    "test_df['Sentiment_pred'] = test_predict\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we classified the sentiment of tweets in test set, next is to classify the extreme tweets. We'll keep the possible neutral tweet corpus as is.\n",
    "\n",
    "## Extreme Text Classifier\n",
    "\n",
    "## Extremely positive  tweet classifier\n",
    "\n",
    "Traing classifier to identify extreme tweets in the previously classified positive tweets.\n",
    "\n",
    "### Tokenize corpus\n",
    "\n",
    "Tokenize Extremely Positive and Positive corpus in training set and transform test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21450\n"
     ]
    }
   ],
   "source": [
    "train = train_df[train_df.Sentiment_encode==2]\n",
    "test = test_df[test_df.Sentiment_pred==2]\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train.CleanTweet.values)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)\n",
    "\n",
    "max_len = 25\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train.CleanTweet.values)\n",
    "train_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test.CleanTweet.values)\n",
    "test_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train.Extreme_txt_encode.values\n",
    "test_target = test.Extreme_txt_encode.values\n",
    "target_len = len(set(extreme_txt_encoder.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18046,), (18046, 25), (1708,), (1708, 25))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape, train_corpus.shape, test_target.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18046 samples, validate on 1708 samples\n",
      "Epoch 1/3\n",
      "18046/18046 [==============================] - 17s 964us/sample - loss: 0.5441 - accuracy: 0.7332 - val_loss: 0.4766 - val_accuracy: 0.7799\n",
      "Epoch 2/3\n",
      "18046/18046 [==============================] - 13s 744us/sample - loss: 0.3619 - accuracy: 0.8496 - val_loss: 0.4858 - val_accuracy: 0.7728\n",
      "Epoch 3/3\n",
      "18046/18046 [==============================] - 14s 753us/sample - loss: 0.2603 - accuracy: 0.8983 - val_loss: 0.5462 - val_accuracy: 0.7711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2023f8538c8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_dim = 32\n",
    "batch = 32\n",
    "epoch = 3\n",
    "\n",
    "extreme_pos_classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embd_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embd_dim*8)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "extreme_pos_classifier.compile(optimizer='adam', loss=tf.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "extreme_pos_classifier.fit(train_corpus, train_target, epochs=epoch, batch_size=batch, validation_data=(test_corpus, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Use trained model to classify the extreme tweets in previously classified positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>Extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>Sentiment_pred</th>\n",
       "      <th>Extreme_pos_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>us find hand sanitizer fred meyer turned pack purell check concerns driving prices</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and loved ones from #coronavirus. ?</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>find protect loved ones</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;amp;medical supplies after...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Location     TweetAt  \\\n",
       "UserName                                    \n",
       "1                         NYC  02-03-2020   \n",
       "2                 Seattle, WA  02-03-2020   \n",
       "3                         NaN  02-03-2020   \n",
       "4                 Chicagoland  02-03-2020   \n",
       "5         Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...   \n",
       "2         When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...   \n",
       "3                                   Find out how you can protect yourself and loved ones from #coronavirus. ?   \n",
       "4         #Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after...   \n",
       "5         #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...   \n",
       "\n",
       "                   Sentiment  Sentiment_encode  Extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "1         Extremely Negative                 0                   1   \n",
       "2                   Positive                 2                   0   \n",
       "3         Extremely Positive                 2                   1   \n",
       "4                   Negative                 0                   0   \n",
       "5                    Neutral                 1                   0   \n",
       "\n",
       "                                                                                                   CleanTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...   \n",
       "2                          us find hand sanitizer fred meyer turned pack purell check concerns driving prices   \n",
       "3                                                                                     find protect loved ones   \n",
       "4          buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...   \n",
       "5                 au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper   \n",
       "\n",
       "          Sentiment_pred Extreme_pos_pred  \n",
       "UserName                                   \n",
       "1                      0             None  \n",
       "2                      2                0  \n",
       "3                      2                1  \n",
       "4                      0             None  \n",
       "5                      1             None  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = [int(np.where(pred>0.5, 1, 0)) for pred in extreme_pos_classifier.predict(test_corpus)]\n",
    "test_df[\"Extreme_pos_pred\"]=None\n",
    "test_df.loc[test_df[test_df.Sentiment_pred==2].index, \"Extreme_pos_pred\"] = test_predict\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely negative tweet classifier\n",
    "\n",
    "Traing classifier to identify extreme tweets in the previously classified negative tweets.\n",
    "\n",
    "### Tokenize corpus\n",
    "\n",
    "Tokenize Extremely Negative and Negative corpus in training set and transform test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20042\n"
     ]
    }
   ],
   "source": [
    "train = train_df[train_df.Sentiment_encode==0]\n",
    "test = test_df[test_df.Sentiment_pred==0]\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train.CleanTweet.values)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)\n",
    "\n",
    "max_len = 25\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train.CleanTweet.values)\n",
    "train_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test.CleanTweet.values)\n",
    "test_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train.Extreme_txt_encode.values\n",
    "test_target = test.Extreme_txt_encode.values\n",
    "target_len = len(set(extreme_txt_encoder.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15398,), (15398, 25), (1543,), (1543, 25))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape, train_corpus.shape, test_target.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15398 samples, validate on 1543 samples\n",
      "Epoch 1/3\n",
      "15398/15398 [==============================] - 15s 1ms/sample - loss: 0.5685 - accuracy: 0.7055 - val_loss: 0.5856 - val_accuracy: 0.7226\n",
      "Epoch 2/3\n",
      "15398/15398 [==============================] - 11s 743us/sample - loss: 0.3691 - accuracy: 0.8402 - val_loss: 0.5430 - val_accuracy: 0.7362\n",
      "Epoch 3/3\n",
      "15398/15398 [==============================] - 12s 747us/sample - loss: 0.2459 - accuracy: 0.9009 - val_loss: 0.6080 - val_accuracy: 0.7447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2032a466b48>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_dim = 32\n",
    "batch = 32\n",
    "epoch = 3\n",
    "\n",
    "extreme_neg_classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embd_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embd_dim*8)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "extreme_neg_classifier.compile(optimizer='adam', loss=tf.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "extreme_neg_classifier.fit(train_corpus, train_target, epochs=epoch, batch_size=batch, validation_data=(test_corpus, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Use trained model to classify the extreme tweets in previously classified negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>Extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>Sentiment_pred</th>\n",
       "      <th>Extreme_pos_pred</th>\n",
       "      <th>Extreme_neg_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>us find hand sanitizer fred meyer turned pack purell check concerns driving prices</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and loved ones from #coronavirus. ?</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>find protect loved ones</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;amp;medical supplies after...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Location     TweetAt  \\\n",
       "UserName                                    \n",
       "1                         NYC  02-03-2020   \n",
       "2                 Seattle, WA  02-03-2020   \n",
       "3                         NaN  02-03-2020   \n",
       "4                 Chicagoland  02-03-2020   \n",
       "5         Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...   \n",
       "2         When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...   \n",
       "3                                   Find out how you can protect yourself and loved ones from #coronavirus. ?   \n",
       "4         #Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after...   \n",
       "5         #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...   \n",
       "\n",
       "                   Sentiment  Sentiment_encode  Extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "1         Extremely Negative                 0                   1   \n",
       "2                   Positive                 2                   0   \n",
       "3         Extremely Positive                 2                   1   \n",
       "4                   Negative                 0                   0   \n",
       "5                    Neutral                 1                   0   \n",
       "\n",
       "                                                                                                   CleanTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...   \n",
       "2                          us find hand sanitizer fred meyer turned pack purell check concerns driving prices   \n",
       "3                                                                                     find protect loved ones   \n",
       "4          buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...   \n",
       "5                 au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper   \n",
       "\n",
       "          Sentiment_pred Extreme_pos_pred Extreme_neg_pred  \n",
       "UserName                                                    \n",
       "1                      0             None                0  \n",
       "2                      2                0             None  \n",
       "3                      2                1             None  \n",
       "4                      0             None                0  \n",
       "5                      1             None             None  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = [int(np.where(pred>0.5, 1, 0)) for pred in extreme_neg_classifier.predict(test_corpus)]\n",
    "test_df[\"Extreme_neg_pred\"]=None\n",
    "test_df.loc[test_df[test_df.Sentiment_pred==0].index, \"Extreme_neg_pred\"] = test_predict\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Find the total misclassified rate and misclassified rate of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Misclassified</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sentiment          Prediction  Misclassified\n",
       "UserName                                                       \n",
       "1         Extremely Negative            Negative              1\n",
       "2                   Positive            Positive              0\n",
       "3         Extremely Positive  Extremely Positive              0\n",
       "4                   Negative            Negative              0\n",
       "5                    Neutral             Neutral              0\n",
       "6                    Neutral             Neutral              0\n",
       "7                   Positive            Positive              0\n",
       "8                    Neutral            Negative              1\n",
       "9         Extremely Negative  Extremely Negative              0\n",
       "10        Extremely Positive            Positive              1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = []\n",
    "for user in test_df.index.values:\n",
    "    sentiment = \"\"\n",
    "    if test_df.loc[user, \"Extreme_pos_pred\"] == 1 or \\\n",
    "       test_df.loc[user, \"Extreme_neg_pred\"] == 1:\n",
    "        sentiment += \"Extremely \"\n",
    "    if test_df.loc[user, \"Sentiment_pred\"] == 0:\n",
    "        sentiment += \"Negative\"\n",
    "    elif test_df.loc[user, \"Sentiment_pred\"] == 1:\n",
    "        sentiment += \"Neutral\"\n",
    "    elif test_df.loc[user, \"Sentiment_pred\"] == 2:\n",
    "        sentiment += \"Positive\"\n",
    "    prediction.append(sentiment)\n",
    "        \n",
    "test_df[\"Prediction\"] = prediction\n",
    "test_df[\"Misclassified\"] = (test_df.Sentiment!=test_df.Prediction).astype(int)\n",
    "test_df[[\"Sentiment\", \"Prediction\", \"Misclassified\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Extremely Negative</th>\n",
       "      <td>592.0</td>\n",
       "      <td>0.380068</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extremely Positive</th>\n",
       "      <td>599.0</td>\n",
       "      <td>0.347245</td>\n",
       "      <td>0.476492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>1041.0</td>\n",
       "      <td>0.409222</td>\n",
       "      <td>0.491927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>619.0</td>\n",
       "      <td>0.234249</td>\n",
       "      <td>0.423871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.287223</td>\n",
       "      <td>0.452706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count      mean       std  min  25%  50%  75%  max\n",
       "Sentiment                                                              \n",
       "Extremely Negative   592.0  0.380068  0.485814  0.0  0.0  0.0  1.0  1.0\n",
       "Extremely Positive   599.0  0.347245  0.476492  0.0  0.0  0.0  1.0  1.0\n",
       "Negative            1041.0  0.409222  0.491927  0.0  0.0  0.0  1.0  1.0\n",
       "Neutral              619.0  0.234249  0.423871  0.0  0.0  0.0  0.0  1.0\n",
       "Positive             947.0  0.287223  0.452706  0.0  0.0  0.0  1.0  1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby([\"Sentiment\"]).Misclassified.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sentiment</th>\n",
       "      <th>Extremely Negative</th>\n",
       "      <th>Extremely Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Extremely Negative</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extremely Positive</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>193.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>23.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Sentiment           Extremely Negative  Extremely Positive  Negative  Neutral  \\\n",
       "Prediction                                                                      \n",
       "Extremely Negative                 0.0                 7.0     164.0      1.0   \n",
       "Extremely Positive                 4.0                 0.0      31.0      9.0   \n",
       "Negative                         193.0                15.0       0.0     72.0   \n",
       "Neutral                            5.0                 NaN      47.0      0.0   \n",
       "Positive                          23.0               186.0     184.0     63.0   \n",
       "\n",
       "Sentiment           Positive  \n",
       "Prediction                    \n",
       "Extremely Negative      21.0  \n",
       "Extremely Positive     142.0  \n",
       "Negative                88.0  \n",
       "Neutral                 21.0  \n",
       "Positive                 0.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the misclassified\n",
    "test_df.pivot_table(index=\"Prediction\", columns=\"Sentiment\", values=\"Misclassified\", aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sentiment</th>\n",
       "      <th>Extremely Negative</th>\n",
       "      <th>Extremely Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Extremely Negative</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.168614</td>\n",
       "      <td>15.754083</td>\n",
       "      <td>0.161551</td>\n",
       "      <td>2.217529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extremely Positive</th>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.977906</td>\n",
       "      <td>1.453958</td>\n",
       "      <td>14.994720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>32.601351</td>\n",
       "      <td>2.504174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.631664</td>\n",
       "      <td>9.292503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>0.844595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.514890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.217529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>3.885135</td>\n",
       "      <td>31.051753</td>\n",
       "      <td>17.675312</td>\n",
       "      <td>10.177706</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Sentiment           Extremely Negative  Extremely Positive   Negative  \\\n",
       "Prediction                                                              \n",
       "Extremely Negative            0.000000            1.168614  15.754083   \n",
       "Extremely Positive            0.675676            0.000000   2.977906   \n",
       "Negative                     32.601351            2.504174   0.000000   \n",
       "Neutral                       0.844595                 NaN   4.514890   \n",
       "Positive                      3.885135           31.051753  17.675312   \n",
       "\n",
       "Sentiment             Neutral   Positive  \n",
       "Prediction                                \n",
       "Extremely Negative   0.161551   2.217529  \n",
       "Extremely Positive   1.453958  14.994720  \n",
       "Negative            11.631664   9.292503  \n",
       "Neutral              0.000000   2.217529  \n",
       "Positive            10.177706   0.000000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rate of misclassification\n",
    "total = test_df.groupby(['Sentiment']).size().to_list()\n",
    "(test_df.pivot_table(index=\"Prediction\", columns=\"Sentiment\", values=\"Misclassified\", aggfunc=np.sum)/total)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "- The overall accuracy: 66% accuracy\n",
    "    - 1276 misclassifications out of 3798 samples\n",
    "    - Neutral (77%) > Positive (71%) > Extremely Negative (65%) > Extremely Negative (62%) > Negative (59%)\n",
    "- Sentiment classifier misclassified:\n",
    "    - 294 out of 1633 negative (Negative + Extremely Negative) tweets were misclassified as other sentiment\n",
    "        - 82% accuracy\n",
    "    - 152 out of 1546 positive (Positive + Extremely Positive) tweets were misclassified as other sentiment\n",
    "        - 90% accuracy\n",
    "    - 145 out of 619 neutral tweets were misclassified as other sentiment\n",
    "        - 77% accuracy\n",
    "\n",
    "- Extremely negative tweet classifier misclassified:\n",
    "    - 164 negative misclassified as extremely negative and 193 extremely negative as negative\n",
    "        - 357 misclassifications out of 1633 negative tweets\n",
    "        - 78% accuracy\n",
    "\n",
    "- Extremely positive tweet classifier misclassified:\n",
    "    - 142 positive misclassified as extremely positive and 186 extremely positive as positive\n",
    "        - 328 misclassifications out of 1546 positive tweets\n",
    "        - 79% accuracy\n",
    "\n",
    "## Conclusion\n",
    "In general, similar observation from first notebook can still be observed here. And applying multiple classified specified with different focus has not helped improving accuracy.\n",
    "1. Independent classifier accuracy v.s. overall accuracy\n",
    "Each classifier independently has better performance, especially the sentiment classifier, but the overall accuracy is worst than first approach since the error cumulates through two predictions back to back. \n",
    "2. Weakness with extreme tweets\n",
    "The result shows that classifier is still unable to fully identify extreme sentiment v.s. normal sentiment even with a dedicated classifier just to detect this feature, which was meant to fix this weakness. It's good idea to review and filter the OriginalTweets and see what keywords or criteria make a tweet extremely positive or negative, then adjust data processing or modeling accordingly.\n",
    "3. Sentiment classification\n",
    "Regardless of extremeness in a tweet, the sentiment classifier by itself has good performance, which is expected since it's observed in the other notebook with other approach that model is weak at detecting extreme vs normal sentiment but relatively good at detecting positive, neutral, and negative. Thus it's expected to have better accuracy if intensity of sentiment is not part of consideration.\n",
    "\n",
    "### Improvements:\n",
    "- Make use of date\n",
    "- Review tweet contents and tokens to clean up text more\n",
    "- Expand stopwords, stemming dictionary, and lemmatization dictionary\n",
    "- Clean up tweets and remove noise so that critical keywords stand out more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
