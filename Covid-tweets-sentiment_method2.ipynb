{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: COVID Tweet Sentiment Classification - Alternative Approach\n",
    "\n",
    "This is a solution for Kaggle project: https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n",
    "\n",
    "This notebook takes different approach based on observations in Covid-tweets-sentiment_method1 notebook.\n",
    "\n",
    "Twitter is a place to express thoughts and ideas and spread news and information, thus makes it a great source to track trends worldwide. Through the collected tweets, we can evaluate people's attitude toward Covid and pandemic through the trending keywords and tags through time. The estimator can be useful for identifying inappropriate tweets and stop toxic propaganda or evaluating strategies to promote social distancing using trend words.\n",
    "\n",
    "The task is to classify tweet sentiments in given tweets.\n",
    "\n",
    "This notebook takes the approach of training multiple classifiers to identify tweet sentiment:\n",
    "1. Sentiment classifier: combine extremely positive with positive and extremely negative with negative, and classify positive, negative, and neutral tweets.\n",
    "2. Extremely positive classifier: from the predicted positive tweets, classify extreme tweets.\n",
    "3. Extremely negative classifier: from the predicted negative tweets, classify extreme tweets.\n",
    "\n",
    "Data has following columns:\n",
    "\n",
    "|Column |Notes |\n",
    "|:------|:-------|\n",
    "UserName | User ID\n",
    "ScreenName| User display name\n",
    "Location | User location\n",
    "TweetAt | Date\n",
    "OriginalTweet | Tweet content\n",
    "Sentiment | Extremely negative, Negative, Neutral, Positive, Extremely positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Corona_NLP_train.csv\", header=0, encoding='latin1')\n",
    "test_df = pd.read_csv(\"Corona_NLP_test.csv\", header=0, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "### Handle useless column\n",
    "Both UserName and ScreenName identify a user. We don't need both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=\"ScreenName\", inplace=True)\n",
    "test_df.drop(columns=\"ScreenName\", inplace=True)\n",
    "train_df.set_index(\"UserName\", inplace=True)\n",
    "test_df.set_index(\"UserName\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode sentiment\n",
    "Two encoders needed:\n",
    "1. Sentiment encoder: encodes Extremely Negative with same label as Negative and Extremely Positive with same label as Positive\n",
    "2. Extreme text encoder: encodes extreme tweets as 1 and regular tweets as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_encoder = {'Extremely Negative': 0,\n",
    "                     'Negative': 0,\n",
    "                     'Neutral': 1, \n",
    "                     'Positive': 2,\n",
    "                     'Extremely Positive': 2}\n",
    "           \n",
    "train_df[\"Sentiment_encode\"] = train_df.Sentiment.map(sentiment_encoder)\n",
    "test_df[\"Sentiment_encode\"] = test_df.Sentiment.map(sentiment_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_txt_encoder = {'Extremely Negative': 1,\n",
    "                      'Negative': 0,\n",
    "                      'Neutral': 0,  \n",
    "                      'Positive': 0,\n",
    "                      'Extremely Positive': 1}\n",
    "\n",
    "train_df[\"Extreme_txt_encode\"] = train_df.Sentiment.map(extreme_txt_encoder)\n",
    "test_df[\"Extreme_txt_encode\"] = test_df.Sentiment.map(extreme_txt_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Account Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep most frquent @:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = []\n",
    "for tweet in train_df.index.values:\n",
    "    tag_list += re.findall(r'@\\w+', train_df.loc[tweet, \"OriginalTweet\"])\n",
    "    \n",
    "tag_df = pd.DataFrame(tag_list)\n",
    "tag_sort = pd.DataFrame(tag_df.value_counts(ascending=False))\n",
    "\n",
    "top_tags = [acct[0] for acct in tag_sort.index.values[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep most frequent hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hash_list = []\n",
    "for tweet in train_df.index.values:\n",
    "    hash_list += re.findall(r'#\\w+', train_df.loc[tweet, \"OriginalTweet\"])\n",
    "    \n",
    "hash_df = pd.DataFrame(hash_list)\n",
    "hash_sort = pd.DataFrame(hash_df.value_counts(ascending=False))\n",
    "\n",
    "top_hashtags = [tag[0] for tag in hash_sort.index.values[:30] \n",
    "                if \"covid\" not in tag[0].lower() and \"corona\" not in tag[0].lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jenny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = list(set(nltk.corpus.stopwords.words('english')))\n",
    "stop_words += [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "               \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "stop_words += [\"in\", \"on\", \"at\", \"via\", \"due\", \"one\", \"two\", \"etc\", \"per\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine tokens\n",
    "After one round of training, examine and normalize tokens. \n",
    "\n",
    "1. Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n",
    "    - Example: running → run\n",
    "2. Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma.\n",
    "    - Example: better → good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_dict = {\"buying\":\"buy\", \"bought\":\"buy\", \"working\":\"work\", \"worked\":\"work\", \"shopping\":\"shop\", \"shopped\":\"shop\",\n",
    "                 \"shops\":\"shop\", \"days\":\"day\", \"weeks\":\"week\", \"masks\":\"mask\", \"got\":\"get\", \"gets\":\"get\", \n",
    "                 \"supermarkets\":\"supermarket\", \"says\":\"say\", \"saying\":\"say\", \"said\":\"say\", \"getting\":\"get\", \"gets\":\"get\", \n",
    "                 \"got\":\"get\", \"making\":\"make\", \"made\":\"make\", \"services\":\"service\", \"hours\":\"hour\", \"years\":\"year\", \n",
    "                 \"increases\":\"increase\", \"increased\":\"increase\", \"markets\":\"market\", \"close\":\"closed\", \"needs\":\"need\",\n",
    "                 \"needed\":\"need\", \"hands\":\"hand\", \"stores\":\"store\", \"employees\":\"employee\", \"workers\":\"worker\", \n",
    "                 \"staffs\":\"staff\", \"businesses\":\"business\", \"companies\":\"company\", \"consumers\":\"consumer\", \n",
    "                 \"customers\":\"customer\", \"products\":\"product\", \"going\":\"go\", \"went\":\"go\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization_dict = {\"shop\":\"store\", \"supermarket\":\"market\", \"much\":\"many\", \"employee\":\"worker\", \"staff\":\"worker\", \n",
    "                      \"global\":\"world\", \"company\":\"business\", \"consumer\":\"customer\", \"house\":\"home\", \"grocery\":\"goods\",\n",
    "                      \"products\":\"goods\", \"toilet\":\"toiletpaper\", \"paper\":\"toiletpaper\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below files are used for data cleaning - Retrieving countries with help of city codes/state codes...(optional)\n",
    "states = json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/states.json\").text)\n",
    "countries=json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/countries.json\").text)\n",
    "cities=json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/cities.json\").text)\n",
    "us_states_code=pd.read_csv('https://worldpopulationreview.com/static/states/abbr-name.csv',names=['state_code','state'])\n",
    "\n",
    "states = states[\"states\"]\n",
    "countries = countries[\"countries\"]\n",
    "cities = cities[\"cities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "world_dict = {}\n",
    "    \n",
    "for city in cities:\n",
    "    state_id = int(re.findall(r'\\d+', city[\"state_id\"])[0])\n",
    "    country_id = states[state_id-1][\"country_id\"]\n",
    "    country_id = int(re.findall(r'\\d+', country_id)[0])\n",
    "    country_name = countries[country_id-1][\"sortname\"]\n",
    "    world_dict[city[\"name\"].lower()] = country_name.lower()\n",
    "    \n",
    "for state in states:\n",
    "    country_id = int(re.findall(r'\\d+', state[\"country_id\"])[0])\n",
    "    country_name = countries[country_id-1][\"sortname\"]\n",
    "    world_dict[state[\"name\"].lower()] = country_name.lower()\n",
    "    \n",
    "for country in countries:    \n",
    "    world_dict[country[\"sortname\"].lower()] = country[\"sortname\"].lower()\n",
    "    world_dict[country[\"name\"].lower()] = country[\"sortname\"].lower()\n",
    "    \n",
    "for index in us_states_code.index:\n",
    "    state = us_states_code.loc[index]\n",
    "    world_dict[state.state_code.lower()] = \"us\"\n",
    "    world_dict[state.state.lower()] = \"us\"\n",
    "    \n",
    "world_dict[\"uk\"] = \"gb\"\n",
    "world_dict[\"ny\"] = \"us\"\n",
    "world_dict[\"nyc\"] = \"us\"\n",
    "world_dict[\"la\"] = \"us\"\n",
    "world_dict[\"sf\"] = \"us\"\n",
    "world_dict[\"bc\"] = \"ca\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean sentence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_link(sentence):\n",
    "    return re.sub(r'http\\S+', \" \", sentence)\n",
    "\n",
    "def remove_tag(sentence):\n",
    "    keep_tag = []\n",
    "    for tag in top_tags:\n",
    "        keep_tag += re.findall(tag, sentence)        \n",
    "    sentence = re.sub(r'@\\S+', \" \", sentence)    \n",
    "    for tag in keep_tag:\n",
    "        sentence += \" \" + tag        \n",
    "    return sentence\n",
    "    \n",
    "def remove_hashtag(sentence):\n",
    "    keep_tag = []\n",
    "    for tag in top_hashtags:\n",
    "        keep_tag += re.findall(tag, sentence)        \n",
    "    sentence = re.sub(r'#\\S+', \" \", sentence)    \n",
    "    for tag in keep_tag:\n",
    "        sentence += \" \" + tag        \n",
    "    return sentence\n",
    "    \n",
    "def remove_special_char(sentence):\n",
    "    for special_ch in (string.punctuation + \"0123456789\" + \"\\r\" + \"\\n\"):\n",
    "        sentence = sentence.replace(special_ch, \" \")\n",
    "    return sentence\n",
    "\n",
    "def remove_non_english(sentence):\n",
    "    clean_sentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        clean_sentence += str(np.where(word.isalpha(), (word + \" \"), \"\"))\n",
    "    return clean_sentence\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    for word in [\"covid\", \"corona\", \"virus\"]:\n",
    "        sentence = re.sub(word, \" \", sentence)\n",
    "    sentence = [word for word in sentence.split() if word not in stop_words]\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "def stemming_and_lemmatization(sentence):\n",
    "    clean_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word in stemming_dict:\n",
    "            word = stemming_dict[word]\n",
    "        if word in lemmatization_dict:\n",
    "            word = lemmatization_dict[word]\n",
    "        clean_sentence.append(word)    \n",
    "    return \" \".join(clean_sentence)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = remove_link(sentence)\n",
    "    sentence = remove_tag(sentence)\n",
    "    sentence = remove_hashtag(sentence)\n",
    "    sentence = remove_special_char(sentence)\n",
    "    sentence = remove_non_english(sentence)\n",
    "    sentence = remove_stop_words(sentence)\n",
    "    sentence = stemming_and_lemmatization(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean OriginalTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_original_tweet(df):\n",
    "    clean_tweet = []\n",
    "    loc_final_list = []\n",
    "\n",
    "    for tweet in df.index.values:\n",
    "        # clean location\n",
    "        loc_final, loc_clean = \"\", \"\"\n",
    "        if not pd.isnull(df.Location.loc[tweet]):\n",
    "            loc_clean = df.Location.astype(str).loc[tweet].lower()\n",
    "            loc_clean = clean_sentence(loc_clean)\n",
    "            for sub in loc_clean.split():\n",
    "                if sub in world_dict:\n",
    "                    loc_final = world_dict[sub]\n",
    "                    break\n",
    "            # add to list if not empty\n",
    "            if loc_final:\n",
    "                loc_final_list.append(loc_final)\n",
    "\n",
    "        # clean message\n",
    "        msg = df.OriginalTweet.astype(str).loc[tweet].lower()\n",
    "        msg = clean_sentence(msg)\n",
    "\n",
    "        # combine location and message to one sentence\n",
    "        clean_tweet.append(loc_final + \" \" + msg)\n",
    "\n",
    "    df['CleanTweet'] = clean_tweet\n",
    "    \n",
    "    return clean_tweet, loc_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_tweet, loc_final_list = clean_original_tweet(train_df)\n",
    "dummy = clean_original_tweet(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_encode</th>\n",
       "      <th>extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to exchange phone numbers create contact list with phone n...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>gb advice talk neighbours family exchange phone numbers create contact list phone numbers neighb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>australia woolworths give elderly disabled dedicated store hour amid outbreak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL B...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>food stock empty please panic enough food everyone take need stay calm stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ready go market outbreak paranoid food stock litteraly empty serious thing please panic causes ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Location     TweetAt  \\\n",
       "UserName                          \n",
       "3799         London  16-03-2020   \n",
       "3800             UK  16-03-2020   \n",
       "3801      Vagabonds  16-03-2020   \n",
       "3802            NaN  16-03-2020   \n",
       "3803            NaN  16-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "3799      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://...   \n",
       "3800      advice Talk to your neighbours family to exchange phone numbers create contact list with phone n...   \n",
       "3801      Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-...   \n",
       "3802      My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL B...   \n",
       "3803      Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid...   \n",
       "\n",
       "                   Sentiment  sentiment_encode  extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "3799                 Neutral                 1                   0   \n",
       "3800                Positive                 2                   0   \n",
       "3801                Positive                 2                   0   \n",
       "3802                Positive                 2                   0   \n",
       "3803      Extremely Negative                 0                   1   \n",
       "\n",
       "                                                                                                   CleanTweet  \n",
       "UserName                                                                                                       \n",
       "3799                                                                                                      gb   \n",
       "3800      gb advice talk neighbours family exchange phone numbers create contact list phone numbers neighb...  \n",
       "3801                            australia woolworths give elderly disabled dedicated store hour amid outbreak  \n",
       "3802                         food stock empty please panic enough food everyone take need stay calm stay safe  \n",
       "3803       ready go market outbreak paranoid food stock litteraly empty serious thing please panic causes ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_encode</th>\n",
       "      <th>extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>us find hand sanitizer fred meyer turned pack purell check concerns driving prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and loved ones from #coronavirus. ?</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>find protect loved ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;amp;medical supplies after...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Location     TweetAt  \\\n",
       "UserName                                    \n",
       "1                         NYC  02-03-2020   \n",
       "2                 Seattle, WA  02-03-2020   \n",
       "3                         NaN  02-03-2020   \n",
       "4                 Chicagoland  02-03-2020   \n",
       "5         Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...   \n",
       "2         When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...   \n",
       "3                                   Find out how you can protect yourself and loved ones from #coronavirus. ?   \n",
       "4         #Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after...   \n",
       "5         #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...   \n",
       "\n",
       "                   Sentiment  sentiment_encode  extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "1         Extremely Negative                 0                   1   \n",
       "2                   Positive                 2                   0   \n",
       "3         Extremely Positive                 2                   1   \n",
       "4                   Negative                 0                   0   \n",
       "5                    Neutral                 1                   0   \n",
       "\n",
       "                                                                                                   CleanTweet  \n",
       "UserName                                                                                                       \n",
       "1         us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...  \n",
       "2                          us find hand sanitizer fred meyer turned pack purell check concerns driving prices  \n",
       "3                                                                                     find protect loved ones  \n",
       "4          buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...  \n",
       "5                 au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.700e+01, 5.670e+02, 1.715e+03, 2.766e+03, 3.604e+03, 3.527e+03,\n",
       "        3.688e+03, 3.801e+03, 3.982e+03, 4.312e+03, 4.333e+03, 3.704e+03,\n",
       "        2.586e+03, 1.465e+03, 6.560e+02, 2.540e+02, 8.400e+01, 2.300e+01,\n",
       "        1.100e+01, 2.000e+00]),\n",
       " array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22., 24.,\n",
       "        26., 28., 30., 32., 34., 36., 38., 40.]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_tweet = [len(tweet.split()) for tweet in train_df.CleanTweet]\n",
    "plt.figure()\n",
    "plt.hist(len_tweet, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5.,  37., 127., 210., 254., 317., 305., 375., 353., 375., 416.,\n",
       "        344., 335., 207.,  79.,  33.,  17.,   7.,   1.,   1.]),\n",
       " array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22., 24.,\n",
       "        26., 28., 30., 32., 34., 36., 38., 40.]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAASoElEQVR4nO3db4hd933n8fdnVa8TkpTY9dirSKLjDQpUDrtKGbQFL0sap7XWDpFTcJChQQsG5YHMJrTQSi20zgOBdsmf9kETUBJTtc1GFSTBwm63VdSGYChWRo7iWFa81taqPZGQpklDoifalfztgzlub6Q7M3fmztUd/fR+weWe87u/c+9XP0afOfO750+qCklSW/7NuAuQJK08w12SGmS4S1KDDHdJapDhLkkN+plxFwBwxx131OTk5LjLkKQbyvHjx/+xqib6vbYqwn1ycpLp6elxlyFJN5Qk/zDfa07LSFKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg1bFGarSaja5++llb3tm34MrWIk0OPfcJalBhrskNchwl6QGGe6S1KCBwz3JmiTfTvJUt357kiNJXu6eb+vpuyfJ6SQvJbl/FIVLkua3lD33jwGnetZ3A0eraiNwtFsnySZgO3APsBX4bJI1K1OuJGkQA4V7kvXAg8AXepq3AQe65QPAQz3tB6vqUlW9ApwGtqxItZKkgQy65/4HwG8Br/e03VVV5wC65zu79nXAaz39Zrq2n5JkZ5LpJNOzs7NLrVuStIBFwz3JB4ALVXV8wPdMn7a6pqFqf1VNVdXUxETfWwBKkpZpkDNU7wU+mOQB4E3Azyb5M+B8krVVdS7JWuBC138G2NCz/Xrg7EoWLUla2KJ77lW1p6rWV9Ukc1+U/k1V/TpwGNjRddsBPNktHwa2J7k1yd3ARuDYilcuSZrXMNeW2QccSvIo8CrwMEBVnUxyCHgRuAzsqqorQ1cqSRrYksK9qr4BfKNb/gFw3zz99gJ7h6xNkrRMnqEqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGuQ9VHXdjPNepMN8tnQjcs9dkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KBB7qH6piTHknwnyckkn+jaH0/y/SQnuscDPdvsSXI6yUtJ7h/lP0CSdK1BzlC9BLyvqi4muQV4Jslfdq99pqo+2ds5ySbmbsd3D/AO4OtJ3uXdmCTp+lk03KuqgIvd6i3doxbYZBtwsKouAa8kOQ1sAf5uyFp1E/PyAdLSDDTnnmRNkhPABeBIVT3bvfRYkueTPJHktq5tHfBaz+YzXZsk6ToZKNyr6kpVbQbWA1uSvBv4HPBOYDNwDvhU1z393uLqhiQ7k0wnmZ6dnV1G6ZKk+SzpaJmq+hFzN8jeWlXnu9B/Hfg8c1MvMLenvqFns/XA2T7vtb+qpqpqamJiYjm1S5LmMcjRMhNJ3t4tvxl4P/C9JGt7un0IeKFbPgxsT3JrkruBjcCxFa1akrSgQY6WWQscSLKGuV8Gh6rqqSR/mmQzc1MuZ4CPAlTVySSHgBeBy8Auj5TRzWqc17DXzW2Qo2WeB97Tp/0jC2yzF9g7XGmSpOXyDFVJapDhLkkN8h6q0irlfL2G4Z67JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGexHQT8uQYqX3uuUtSgwx3SWqQ4S5JDTLcJalBg9xm701JjiX5TpKTST7Rtd+e5EiSl7vn23q22ZPkdJKXktw/yn+AJOlagxwtcwl4X1VdTHIL8EySvwR+DThaVfuS7AZ2A7+dZBOwHbgHeAfw9STv8lZ7bRjmSBtJ18+ie+4152K3ekv3KGAbcKBrPwA81C1vAw5W1aWqegU4DWxZyaIlSQsbaM49yZokJ4ALwJGqeha4q6rOAXTPd3bd1wGv9Ww+07Vd/Z47k0wnmZ6dnR3inyBJutpA4V5VV6pqM7Ae2JLk3Qt0T7+36POe+6tqqqqmJiYmBipWkjSYJR0tU1U/Ar4BbAXOJ1kL0D1f6LrNABt6NlsPnB22UEnS4AY5WmYiydu75TcD7we+BxwGdnTddgBPdsuHge1Jbk1yN7AROLbCdUuSFjDI0TJrgQNJ1jD3y+BQVT2V5O+AQ0keBV4FHgaoqpNJDgEvApeBXR4pI0nX16LhXlXPA+/p0/4D4L55ttkL7B26OknSsniGqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYPcZm9Dkr9NcirJySQf69ofT/L9JCe6xwM92+xJcjrJS0nuH+U/QJJ0rUFus3cZ+M2qei7J24DjSY50r32mqj7Z2znJJmA7cA/wDuDrSd7lrfZWzuTup8ddgqRVbtE996o6V1XPdcs/AU4B6xbYZBtwsKouVdUrwGlgy0oUK0kazJLm3JNMMnc/1We7pseSPJ/kiSS3dW3rgNd6Npuhzy+DJDuTTCeZnp2dXXrlkqR5DRzuSd4KfAX4eFX9GPgc8E5gM3AO+NQbXftsXtc0VO2vqqmqmpqYmFhq3ZKkBQwU7kluYS7Yv1RVXwWoqvNVdaWqXgc+z79OvcwAG3o2Xw+cXbmSJUmLGeRomQBfBE5V1ad72tf2dPsQ8EK3fBjYnuTWJHcDG4FjK1eyJGkxgxwtcy/wEeC7SU50bb8DPJJkM3NTLmeAjwJU1ckkh4AXmTvSZpdHykjS9bVouFfVM/SfR/+LBbbZC+wdoi5J0hA8Q1WSGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWiQ67lrBCZ3Pz3uEiQ1zD13SWrQonvuSTYAfwL8O+B1YH9V/WGS24E/ByaZuxPTh6vqn7pt9gCPAleA/15VfzWS6iX1Nexfhmf2PbhClWhcBtlzvwz8ZlX9AvBLwK4km4DdwNGq2ggc7dbpXtsO3ANsBT6bZM0oipck9bdouFfVuap6rlv+CXAKWAdsAw503Q4AD3XL24CDVXWpql4BTgNbVrhuSdICljTnnmQSeA/wLHBXVZ2DuV8AwJ1dt3XAaz2bzXRtV7/XziTTSaZnZ2eXUbokaT4Dh3uStwJfAT5eVT9eqGuftrqmoWp/VU1V1dTExMSgZUiSBjBQuCe5hblg/1JVfbVrPp9kbff6WuBC1z4DbOjZfD1wdmXKlSQNYtFwTxLgi8Cpqvp0z0uHgR3d8g7gyZ727UluTXI3sBE4tnIlS5IWM8hJTPcCHwG+m+RE1/Y7wD7gUJJHgVeBhwGq6mSSQ8CLzB1ps6uqrqx04ZKk+S0a7lX1DP3n0QHum2ebvcDeIeqSJA3BM1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0a5DZ7TyS5kOSFnrbHk3w/yYnu8UDPa3uSnE7yUpL7R1W4JGl+g+y5/zGwtU/7Z6pqc/f4C4Akm4DtwD3dNp9NsmalipUkDWbRcK+qbwI/HPD9tgEHq+pSVb0CnAa2DFGfJGkZhplzfyzJ8920zW1d2zrgtZ4+M13bNZLsTDKdZHp2dnaIMiRJV1tuuH8OeCewGTgHfKpr73cj7er3BlW1v6qmqmpqYmJimWVIkvpZVrhX1fmqulJVrwOf51+nXmaADT1d1wNnhytRkrRUywr3JGt7Vj8EvHEkzWFge5Jbk9wNbASODVeiJGmpfmaxDkm+DLwXuCPJDPD7wHuTbGZuyuUM8FGAqjqZ5BDwInAZ2FVVV0ZSuSRpXouGe1U90qf5iwv03wvsHaYoSdJwPENVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWjRQyE1v8ndT4+7BEnqyz13SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lq0KLhnuSJJBeSvNDTdnuSI0le7p5v63ltT5LTSV5Kcv+oCpckzW+QPfc/BrZe1bYbOFpVG4Gj3TpJNgHbgXu6bT6bZM2KVStJGsii4V5V3wR+eFXzNuBAt3wAeKin/WBVXaqqV4DTwJaVKVWSNKjlzrnfVVXnALrnO7v2dcBrPf1murZrJNmZZDrJ9Ozs7DLLkCT1s9JfqKZPW/XrWFX7q2qqqqYmJiZWuAxJurktN9zPJ1kL0D1f6NpngA09/dYDZ5dfniRpOZYb7oeBHd3yDuDJnvbtSW5NcjewETg2XImSpKVa9GYdSb4MvBe4I8kM8PvAPuBQkkeBV4GHAarqZJJDwIvAZWBXVV0ZUe2SpHksGu5V9cg8L903T/+9wN5hipIkDcczVCWpQYa7JDXIcJekBhnuktSgRb9QlXTzmdz99LK3PbPvwRWsRMvlnrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgoa4tk+QM8BPgCnC5qqaS3A78OTAJnAE+XFX/NFyZkqSlWIk991+uqs1VNdWt7waOVtVG4Gi3Lkm6jkYxLbMNONAtHwAeGsFnSJIWMGy4F/DXSY4n2dm13VVV5wC65zv7bZhkZ5LpJNOzs7NDliFJ6jXs9dzvraqzSe4EjiT53qAbVtV+YD/A1NRUDVmHJKnHUHvuVXW2e74AfA3YApxPshage74wbJGSpKVZdrgneUuSt72xDPwq8AJwGNjRddsBPDlskZKkpRlmWuYu4GtJ3nif/1VV/zvJt4BDSR4FXgUeHr5MSdJSLDvcq+rvgf/Yp/0HwH3DFCVJGs5Nf4PsYW4ELEmrlZcfkKQG3fR77pJW1jB/DZ/Z9+AKVnJzc89dkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGeZy7pFXDY+RXjnvuktQgw12SGmS4S1KDDHdJapBfqEpqgl/G/rSRhXuSrcAfAmuAL1TVvlF9ltdkl6SfNpJpmSRrgD8C/iuwCXgkyaZRfJYk6Vqj2nPfApzubsVHkoPANuDFEX2eJC3bOP/6H9WU0KjCfR3wWs/6DPCfejsk2Qns7FYvJnlpiM+7A/jHIbYfFetaGutaGutamlVZV/7HUHX9/HwvjCrc06etfmqlaj+wf0U+LJmuqqmVeK+VZF1LY11LY11Lc7PVNapDIWeADT3r64GzI/osSdJVRhXu3wI2Jrk7yb8FtgOHR/RZkqSrjGRapqouJ3kM+CvmDoV8oqpOjuKzOisyvTMC1rU01rU01rU0N1VdqarFe0mSbihefkCSGmS4S1KDbuhwT7I1yUtJTifZPe563pDkTJLvJjmRZHqMdTyR5EKSF3rabk9yJMnL3fNtq6Sux5N8vxuzE0keGENdG5L8bZJTSU4m+VjXPtYxW6CusY5ZkjclOZbkO11dn+jaxz1e89U19p+xro41Sb6d5KlufSTjdcPOuXeXOPg/wK8wd+jlt4BHqmrsZ8EmOQNMVdVYT5hI8l+Ai8CfVNW7u7b/CfywqvZ1vxBvq6rfXgV1PQ5crKpPXs9arqprLbC2qp5L8jbgOPAQ8N8Y45gtUNeHGeOYJQnwlqq6mOQW4BngY8CvMd7xmq+urYz5Z6yr7zeAKeBnq+oDo/o/eSPvuf/LJQ6q6v8Bb1ziQJ2q+ibww6uatwEHuuUDzIXEdTVPXWNXVeeq6rlu+SfAKebOth7rmC1Q11jVnIvd6i3doxj/eM1X19glWQ88CHyhp3kk43Ujh3u/SxyM/Qe+U8BfJzneXWZhNbmrqs7BXGgAd465nl6PJXm+m7a57tNFvZJMAu8BnmUVjdlVdcGYx6ybYjgBXACOVNWqGK956oLx/4z9AfBbwOs9bSMZrxs53Be9xMEY3VtVv8jcVTF3ddMQWtjngHcCm4FzwKfGVUiStwJfAT5eVT8eVx1X61PX2Mesqq5U1WbmzkLfkuTd17uGfuapa6zjleQDwIWqOn49Pu9GDvdVe4mDqjrbPV8AvsbcFNJqcb6bw31jLvfCmOsBoKrOd/8hXwc+z5jGrJuj/Qrwpar6atc89jHrV9dqGbOulh8B32BuXnvs49WvrlUwXvcCH+y+kzsIvC/JnzGi8bqRw31VXuIgyVu6L71I8hbgV4EXFt7qujoM7OiWdwBPjrGWf/HGD3fnQ4xhzLov4r4InKqqT/e8NNYxm6+ucY9Zkokkb++W3wy8H/ge4x+vvnWNe7yqak9Vra+qSeby6m+q6tcZ1XhV1Q37AB5g7oiZ/wv87rjr6Wr698B3usfJcdYFfJm5Pz//P3N/6TwK/BxwFHi5e759ldT1p8B3gee7H/a1Y6jrPzM3tfc8cKJ7PDDuMVugrrGOGfAfgG93n/8C8Htd+7jHa766xv4z1lPje4GnRjleN+yhkJKk+d3I0zKSpHkY7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalB/wwd3ojT5xXIPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_tweet = [len(tweet.split()) for tweet in test_df.CleanTweet]\n",
    "plt.figure()\n",
    "plt.hist(len_tweet, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classifier\n",
    "Classify if tweet is positive, neutral, or negative regardless if tweet uses extreme/drastic tones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32223\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df.CleanTweet.values)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)\n",
    "\n",
    "max_len = 25\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_df.CleanTweet.values)\n",
    "train_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test_df.CleanTweet.values)\n",
    "test_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_df.Sentiment_encode.values\n",
    "test_target = test_df.Sentiment_encode.values\n",
    "target_len = len(set(sentiment_encoder.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41157, 25), (41157,), (3798, 25), (3798,), 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus.shape, train_target.shape, test_corpus.shape, test_target.shape, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15398.,     0.,     0.,     0.,     0.,  7713.,     0.,     0.,\n",
       "            0., 18046.]),\n",
       " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41157 samples, validate on 3798 samples\n",
      "Epoch 1/3\n",
      "41157/41157 [==============================] - 28s 690us/sample - loss: 0.7329 - accuracy: 0.6338 - val_loss: 0.5044 - val_accuracy: 0.8131\n",
      "Epoch 2/3\n",
      "41157/41157 [==============================] - 22s 529us/sample - loss: 0.3646 - accuracy: 0.8760 - val_loss: 0.4431 - val_accuracy: 0.8436\n",
      "Epoch 3/3\n",
      "41157/41157 [==============================] - 23s 560us/sample - loss: 0.2721 - accuracy: 0.9126 - val_loss: 0.4418 - val_accuracy: 0.8489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2565926fb48>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_dim = 32\n",
    "batch = 32\n",
    "epoch = 3\n",
    "\n",
    "sentiment_classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embd_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embd_dim*8)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(target_len, activation=tf.nn.sigmoid)\n",
    "])\n",
    "sentiment_classifier.compile(optimizer='adam', loss=tf.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "sentiment_classifier.fit(train_corpus, train_target, epochs=epoch, batch_size=batch, validation_data=(test_corpus, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_encode</th>\n",
       "      <th>extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>Sentiment_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>us find hand sanitizer fred meyer turned pack purell check concerns driving prices</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and loved ones from #coronavirus. ?</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>find protect loved ones</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;amp;medical supplies after...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Location     TweetAt  \\\n",
       "UserName                                    \n",
       "1                         NYC  02-03-2020   \n",
       "2                 Seattle, WA  02-03-2020   \n",
       "3                         NaN  02-03-2020   \n",
       "4                 Chicagoland  02-03-2020   \n",
       "5         Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...   \n",
       "2         When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...   \n",
       "3                                   Find out how you can protect yourself and loved ones from #coronavirus. ?   \n",
       "4         #Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after...   \n",
       "5         #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...   \n",
       "\n",
       "                   Sentiment  sentiment_encode  extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "1         Extremely Negative                 0                   1   \n",
       "2                   Positive                 2                   0   \n",
       "3         Extremely Positive                 2                   1   \n",
       "4                   Negative                 0                   0   \n",
       "5                    Neutral                 1                   0   \n",
       "\n",
       "                                                                                                   CleanTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...   \n",
       "2                          us find hand sanitizer fred meyer turned pack purell check concerns driving prices   \n",
       "3                                                                                     find protect loved ones   \n",
       "4          buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...   \n",
       "5                 au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper   \n",
       "\n",
       "          Sentiment_pred  \n",
       "UserName                  \n",
       "1                      0  \n",
       "2                      2  \n",
       "3                      2  \n",
       "4                      0  \n",
       "5                      1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = np.argmax(sentiment_classifier.predict(test_corpus), axis=1)\n",
    "test_df['Sentiment_pred'] = test_predict\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we classified the sentiment of tweets in test set, next is to classify the extreme tweets. We'll keep the possible neutral tweet corpus as is.\n",
    "\n",
    "## Extreme Text Classifier\n",
    "\n",
    "## Extremely positive  tweet classifier\n",
    "\n",
    "Traing classifier to identify extreme tweets in the previously classified positive tweets.\n",
    "\n",
    "### Tokenize corpus\n",
    "\n",
    "Tokenize Extremely Positive and Positive corpus in training set and transform test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21450\n"
     ]
    }
   ],
   "source": [
    "train = train_df[train_df.Sentiment_encode==2]\n",
    "test = test_df[test_df.Sentiment_pred==2]\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train.CleanTweet.values)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)\n",
    "\n",
    "max_len = 25\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train.CleanTweet.values)\n",
    "train_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test.CleanTweet.values)\n",
    "test_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train.Extreme_txt_encode.values\n",
    "test_target = test.Extreme_txt_encode.values\n",
    "target_len = len(set(extreme_txt_encoder.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18046,), (18046, 25), (1564,), (1564, 25))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape, train_corpus.shape, test_target.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18046 samples, validate on 1564 samples\n",
      "Epoch 1/3\n",
      "18046/18046 [==============================] - 12s 672us/sample - loss: 0.5474 - accuracy: 0.7269 - val_loss: 0.5144 - val_accuracy: 0.7519\n",
      "Epoch 2/3\n",
      "18046/18046 [==============================] - 8s 449us/sample - loss: 0.3648 - accuracy: 0.8497 - val_loss: 0.4994 - val_accuracy: 0.7609\n",
      "Epoch 3/3\n",
      "18046/18046 [==============================] - 8s 455us/sample - loss: 0.2634 - accuracy: 0.8961 - val_loss: 0.5258 - val_accuracy: 0.7641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25638eb0488>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_dim = 32\n",
    "batch = 32\n",
    "epoch = 3\n",
    "\n",
    "extreme_pos_classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embd_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embd_dim*8)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "extreme_pos_classifier.compile(optimizer='adam', loss=tf.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "extreme_pos_classifier.fit(train_corpus, train_target, epochs=epoch, batch_size=batch, validation_data=(test_corpus, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Use trained model to classify the extreme tweets in previously classified positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_encode</th>\n",
       "      <th>extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>Sentiment_pred</th>\n",
       "      <th>Extreme_pos_pred</th>\n",
       "      <th>Extreme_neg_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>us find hand sanitizer fred meyer turned pack purell check concerns driving prices</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and loved ones from #coronavirus. ?</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>find protect loved ones</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;amp;medical supplies after...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Location     TweetAt  \\\n",
       "UserName                                    \n",
       "1                         NYC  02-03-2020   \n",
       "2                 Seattle, WA  02-03-2020   \n",
       "3                         NaN  02-03-2020   \n",
       "4                 Chicagoland  02-03-2020   \n",
       "5         Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...   \n",
       "2         When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...   \n",
       "3                                   Find out how you can protect yourself and loved ones from #coronavirus. ?   \n",
       "4         #Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after...   \n",
       "5         #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...   \n",
       "\n",
       "                   Sentiment  sentiment_encode  extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "1         Extremely Negative                 0                   1   \n",
       "2                   Positive                 2                   0   \n",
       "3         Extremely Positive                 2                   1   \n",
       "4                   Negative                 0                   0   \n",
       "5                    Neutral                 1                   0   \n",
       "\n",
       "                                                                                                   CleanTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...   \n",
       "2                          us find hand sanitizer fred meyer turned pack purell check concerns driving prices   \n",
       "3                                                                                     find protect loved ones   \n",
       "4          buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...   \n",
       "5                 au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper   \n",
       "\n",
       "          Sentiment_pred Extreme_pos_pred Extreme_neg_pred  \n",
       "UserName                                                    \n",
       "1                      0             None                0  \n",
       "2                      2                0             None  \n",
       "3                      2                1             None  \n",
       "4                      0             None                0  \n",
       "5                      1             None             None  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = [int(np.where(pred>0.5, 1, 0)) for pred in extreme_pos_classifier.predict(test_corpus)]\n",
    "test_df[\"Extreme_pos_pred\"]=None\n",
    "test_df.loc[test_df[test_df.Sentiment_pred==2].index, \"Extreme_pos_pred\"] = test_predict\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely negative tweet classifier\n",
    "\n",
    "Traing classifier to identify extreme tweets in the previously classified negative tweets.\n",
    "\n",
    "### Tokenize corpus\n",
    "\n",
    "Tokenize Extremely Negative and Negative corpus in training set and transform test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20042\n"
     ]
    }
   ],
   "source": [
    "train = train_df[train_df.Sentiment_encode==0]\n",
    "test = test_df[test_df.Sentiment_pred==0]\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train.CleanTweet.values)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)\n",
    "\n",
    "max_len = 25\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train.CleanTweet.values)\n",
    "train_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test.CleanTweet.values)\n",
    "test_corpus = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train.Extreme_txt_encode.values\n",
    "test_target = test.Extreme_txt_encode.values\n",
    "target_len = len(set(extreme_txt_encoder.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15398,), (15398, 25), (1679,), (1679, 25))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape, train_corpus.shape, test_target.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15398 samples, validate on 1679 samples\n",
      "Epoch 1/3\n",
      "15398/15398 [==============================] - 11s 708us/sample - loss: 0.5747 - accuracy: 0.7001 - val_loss: 0.5447 - val_accuracy: 0.7284\n",
      "Epoch 2/3\n",
      "15398/15398 [==============================] - 7s 447us/sample - loss: 0.3808 - accuracy: 0.8350 - val_loss: 0.5763 - val_accuracy: 0.7487\n",
      "Epoch 3/3\n",
      "15398/15398 [==============================] - 7s 445us/sample - loss: 0.2474 - accuracy: 0.9028 - val_loss: 0.6952 - val_accuracy: 0.7338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2573d6221c8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_dim = 32\n",
    "batch = 32\n",
    "epoch = 3\n",
    "\n",
    "extreme_neg_classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embd_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embd_dim*8)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "extreme_neg_classifier.compile(optimizer='adam', loss=tf.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "extreme_neg_classifier.fit(train_corpus, train_target, epochs=epoch, batch_size=batch, validation_data=(test_corpus, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Use trained model to classify the extreme tweets in previously classified negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_encode</th>\n",
       "      <th>extreme_txt_encode</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>Sentiment_pred</th>\n",
       "      <th>Extreme_pos_pred</th>\n",
       "      <th>Extreme_neg_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>us find hand sanitizer fred meyer turned pack purell check concerns driving prices</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and loved ones from #coronavirus. ?</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>find protect loved ones</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;amp;medical supplies after...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Location     TweetAt  \\\n",
       "UserName                                    \n",
       "1                         NYC  02-03-2020   \n",
       "2                 Seattle, WA  02-03-2020   \n",
       "3                         NaN  02-03-2020   \n",
       "4                 Chicagoland  02-03-2020   \n",
       "5         Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                                                                                OriginalTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-...   \n",
       "2         When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack...   \n",
       "3                                   Find out how you can protect yourself and loved ones from #coronavirus. ?   \n",
       "4         #Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after...   \n",
       "5         #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News ...   \n",
       "\n",
       "                   Sentiment  sentiment_encode  extreme_txt_encode  \\\n",
       "UserName                                                             \n",
       "1         Extremely Negative                 0                   1   \n",
       "2                   Positive                 2                   0   \n",
       "3         Extremely Positive                 2                   1   \n",
       "4                   Negative                 0                   0   \n",
       "5                    Neutral                 1                   0   \n",
       "\n",
       "                                                                                                   CleanTweet  \\\n",
       "UserName                                                                                                        \n",
       "1         us trending new yorkers encounter empty market shelves pictured wegmans brooklyn sold online gro...   \n",
       "2                          us find hand sanitizer fred meyer turned pack purell check concerns driving prices   \n",
       "3                                                                                     find protect loved ones   \n",
       "4          buy hits city anxious shoppers stock food amp medical supplies worker becomes st confirmed pati...   \n",
       "5                 au week everyone buy baby milk powder next everyone buy toiletpaper toiletpaper toiletpaper   \n",
       "\n",
       "          Sentiment_pred Extreme_pos_pred Extreme_neg_pred  \n",
       "UserName                                                    \n",
       "1                      0             None                0  \n",
       "2                      2                0             None  \n",
       "3                      2                1             None  \n",
       "4                      0             None                0  \n",
       "5                      1             None             None  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = [int(np.where(pred>0.5, 1, 0)) for pred in extreme_neg_classifier.predict(test_corpus)]\n",
    "test_df[\"Extreme_neg_pred\"]=None\n",
    "test_df.loc[test_df[test_df.Sentiment_pred==0].index, \"Extreme_neg_pred\"] = test_predict\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Find the total misclassified rate and misclassified rate of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenny\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\jenny\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_pred</th>\n",
       "      <th>Extreme_pos_pred</th>\n",
       "      <th>Extreme_neg_pred</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Misclassified</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sentiment  Sentiment_pred Extreme_pos_pred  \\\n",
       "UserName                                                        \n",
       "1         Extremely Negative               0             None   \n",
       "2                   Positive               2                0   \n",
       "3         Extremely Positive               2                1   \n",
       "4                   Negative               0             None   \n",
       "5                    Neutral               1             None   \n",
       "6                    Neutral               1             None   \n",
       "7                   Positive               2                0   \n",
       "8                    Neutral               0             None   \n",
       "9         Extremely Negative               0             None   \n",
       "10        Extremely Positive               2                0   \n",
       "\n",
       "         Extreme_neg_pred          Prediction  Misclassified  \n",
       "UserName                                                      \n",
       "1                       0            Negative              1  \n",
       "2                    None            Positive              0  \n",
       "3                    None  Extremely Positive              0  \n",
       "4                       0            Negative              0  \n",
       "5                    None             Neutral              0  \n",
       "6                    None             Neutral              0  \n",
       "7                    None            Positive              0  \n",
       "8                       0            Negative              1  \n",
       "9                       1  Extremely Negative              0  \n",
       "10                   None            Positive              1  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = []\n",
    "for user in test_eval_df.index.values:\n",
    "    sentiment = \"\"\n",
    "    if test_df.loc[user, \"Extreme_pos_pred\"] == 1 or \\\n",
    "       test_df.loc[user, \"Extreme_neg_pred\"] == 1:\n",
    "        sentiment += \"Extremely \"\n",
    "    if test_df.loc[user, \"Sentiment_pred\"] == 0:\n",
    "        sentiment += \"Negative\"\n",
    "    elif test_df.loc[user, \"Sentiment_pred\"] == 1:\n",
    "        sentiment += \"Neutral\"\n",
    "    elif test_df.loc[user, \"Sentiment_pred\"] == 2:\n",
    "        sentiment += \"Positive\"\n",
    "    prediction.append(sentiment)\n",
    "        \n",
    "test_df[\"Prediction\"] = prediction\n",
    "test_df[\"Misclassified\"] = (test_eval_df.Sentiment!=test_eval_df.Prediction).astype(int)\n",
    "test_df[[\"Sentiment\", \"Prediction\", \"Misclassified\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Extremely Negative</th>\n",
       "      <td>592.0</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.497490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extremely Positive</th>\n",
       "      <td>599.0</td>\n",
       "      <td>0.348915</td>\n",
       "      <td>0.477025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>1041.0</td>\n",
       "      <td>0.332373</td>\n",
       "      <td>0.471290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>619.0</td>\n",
       "      <td>0.232633</td>\n",
       "      <td>0.422852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.326294</td>\n",
       "      <td>0.469104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count      mean       std  min  25%  50%  75%  max\n",
       "Sentiment                                                              \n",
       "Extremely Negative   592.0  0.445946  0.497490  0.0  0.0  0.0  1.0  1.0\n",
       "Extremely Positive   599.0  0.348915  0.477025  0.0  0.0  0.0  1.0  1.0\n",
       "Negative            1041.0  0.332373  0.471290  0.0  0.0  0.0  1.0  1.0\n",
       "Neutral              619.0  0.232633  0.422852  0.0  0.0  0.0  0.0  1.0\n",
       "Positive             947.0  0.326294  0.469104  0.0  0.0  0.0  1.0  1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eval_df.groupby([\"Sentiment\"]).Misclassified.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Extremely Negative    264\n",
       "Extremely Positive    209\n",
       "Negative              346\n",
       "Neutral               144\n",
       "Positive              309\n",
       "Name: Misclassified, dtype: int32"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eval_df.groupby([\"Sentiment\"]).Misclassified.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>Extremely Negative</th>\n",
       "      <th>Extremely Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Extremely Negative</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extremely Positive</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>145.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>26.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction          Extremely Negative  Extremely Positive  Negative  Neutral  \\\n",
       "Sentiment                                                                       \n",
       "Extremely Negative                 0.0                 2.0     243.0      4.0   \n",
       "Extremely Positive                10.0                 0.0      29.0      NaN   \n",
       "Negative                         145.0                34.0       0.0     48.0   \n",
       "Neutral                            4.0                10.0      84.0      0.0   \n",
       "Positive                          26.0               140.0     115.0     28.0   \n",
       "\n",
       "Prediction          Positive  \n",
       "Sentiment                     \n",
       "Extremely Negative      15.0  \n",
       "Extremely Positive     170.0  \n",
       "Negative               119.0  \n",
       "Neutral                 46.0  \n",
       "Positive                 0.0  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eval_df.pivot_table(index=\"Sentiment\", columns=\"Prediction\", values=\"Misclassified\", aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "- The overall accuracy:\n",
    "    - Extremely Negative\n",
    "        - Misclassified 264 out of 592 extremely negative tweets\n",
    "        - 44.59% misclassification rate\n",
    "        - 55.41% accuracy\n",
    "    - Negative\n",
    "        - Misclassified 346 out of 1041 negative tweets\n",
    "        - 33.24% misclassification rate\n",
    "        - 66.76% accuracy\n",
    "    - Neutral\n",
    "        - Misclassified 144 out of 619 neutral tweets\n",
    "        - 23.26% misclassification rate\n",
    "        - 76.74% accuracy\n",
    "    - Positive\n",
    "        - Misclassified 309 out of 947 positive tweets\n",
    "        - 32.63% misclassification rate\n",
    "        - 67.37% accuracy\n",
    "    - Extremely Negative\n",
    "        - Misclassified 209 out of 599 extremely positive tweets\n",
    "        - 34.89% misclassification rate\n",
    "        - 65.11% accuracy\n",
    "\n",
    "- Sentiment classifier misclassified:\n",
    "    - 222 out of 1633 negative (Negative + Extremely Negative) tweets were misclassified as other sentiment\n",
    "        - 13.59% misclassification rate\n",
    "        - 86.41% accuracy\n",
    "    - 208 out of 1546 positive (Positive + Extremely Positive) tweets were misclassified as other sentiment\n",
    "        - 13.45% misclassification rate\n",
    "        - 86.55% accuracy\n",
    "    - 144 out of 619 neutral tweets were misclassified as other sentiment\n",
    "        - 23.26% misclassification rate\n",
    "        - 76.74% accuracy\n",
    "\n",
    "- Extremely negative tweet classifier misclassified:\n",
    "    - 145 negative as extremely negative and 243 extremely negative as negative\n",
    "        - 388 misclassifications out of 1633 negative tweets\n",
    "        - 23.76% misclassification rate\n",
    "        - 76.24% accuracy\n",
    "\n",
    "- Extremely positive tweet classifier misclassified:\n",
    "    - 140 positive as extremely positive and 170 extremely positive as positive\n",
    "        - 310 misclassifications out of 1546 positive tweets\n",
    "        - 20.05% misclassification rate\n",
    "        - 79.95% accuracy\n",
    "\n",
    "## Conclusion\n",
    "1. Independent classifier accuracy v.s. overall accuracy\n",
    "Each classifier independently has good performance, especially sentiment classifier, but the overall accuracy is worst than first approach since the error is cumulated through 2 predictions back to back. \n",
    "2. Weakness of extreme tweets\n",
    "The result shows that classifier is still unable to fully identify extreme sentiment v.s. normal sentiment even with a dedicated classifier just to detect this feature, which was meant to enhance this weakness. It's good idea to review and filter the OriginalTweets and see what keywords or criteria make a tweet extremely positive or negative, then adjust data processing or modeling accordingly.\n",
    "3. Sentiment classification\n",
    "Regardless of extremeness in a tweet, the sentiment classifier by itself has good performance, which is expected since it's observed in the other notebook with other approach that model is weak at detecting extreme vs normal sentiment but relatively good at detecting positive, neutral, and negative. Thus it's expected to have better accuracy if intensity of sentiment is not part of consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
